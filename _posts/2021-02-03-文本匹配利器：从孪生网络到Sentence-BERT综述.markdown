---
layout: post
title:  "文本匹配利器：从孪生网络到Sentence-BERT综述"
date:   2021-02-03 9:00:00 +0530
categories: 自然语言处理 文本匹配 BERT
---

![文本匹配利器：从孪生网络到Sentence-BERT综述](https://pic4.zhimg.com/v2-064fcac77d6716e29880e856722955b4_1440w.jpg?source=172ae18b)

文本匹配是自然语言处理领域一个基础且重要的方向，一般研究两段文本之间的关系。文本相似度、自然语言推理、问答系统、信息检索都可以看作针对不同数据和场景的文本匹配应用。

**本文总结了文本匹配任务中的经典网络Siamse Network，它和近期预训练语言模型的组合，一些调优技巧以及在线下数据集上的效果检验。**

## **1、Siamese 孪生网络**

在正式介绍之前，我们先来看一个有趣的故事。

### **孪生网络的由来**

“Siamese”中的“Siam”是古时泰国的称呼，中文译作暹罗，所以“Siamese”就是指“暹罗”人或“泰国”人。“Siamese”在英语中同时表示“孪生”，这又是为什么呢？

![暹罗连体人](https://pic2.zhimg.com/v2-09a47d0997351739043326aa37892c59_b.png)

十九世纪，泰国出生了一对连体婴儿“恩”和“昌”，当时的医学技术无法使他们分离出来，于是两人顽强地生活了一生。

1829年他们被英国商人发现，进入马戏团，在全世界各地演出。1839年他们访问美国北卡罗莱那州成为“玲玲马戏团” 的台柱，最后成为美国公民。1843年4月13日跟英国一对姐妹结婚，恩生了10个小孩，昌生了12个。1874年，两人因病均于63岁离开了人间。他们的肝至今仍保存在费城的马特博物馆内。

从此之后“暹罗双胞胎”（Siamese twins）就成了连体人的代名词，也因为这对双胞胎全世界开始重视这项特殊疾病。

### **孪生网络**

由于结构具有鲜明的对称性，就像两个孪生兄弟，所以下图这种神经网络结构被研究人员称作“Siamese Network”，即孪生网络。

![img](https://pic2.zhimg.com/v2-3896d1fc52818b0c3c2458f47bc18721_b.png)

其中最能体现“孪生”的地方，在于网络具有相同的编码器（sentence encoder），即将文本转换为高维向量的部分。网络随后对两段文本的特征进行交互，最后完成分类/相似预测。**“孪生网络”结构简单，训练稳定，是很多文本任务不错的baseline模型。**

孪生网络的具体用途是衡量两个输入文本的相似程度。

例如，现在我们有文本1和2，首先把它们分别输入 sentence encoder 进行特征提取和编码，将输入映射到新的空间得到特征向量u和v；最终通过u、v的拼接组合，经过下游网络来计算文本1和2的相似性。

整个过程有2个值得关注的点：

- 在训练和测试中，模型的编码器是权重共享的（“孪生”）；编码器的选择非常广泛，传统的CNN、RNN和Attention、Transformer都可以
- 得到特征u、v后，可以直接使用cosine距离、欧式距离得到两个文本的相似度；不过**更通用的做法**是，基于u和v构建用于匹配两者关系的特征向量，然后用额外的模型学习通用的文本关系映射；毕竟我们的场景不一定只是衡量相似度，可能还有问答、蕴含等复杂任务

### **三连体网络**

基于孪生网络，还有人提出了 Triplet network 三连体网络。顾名思义，输入由三部分组成，文本1，和1相似的文本2，和1不相似的文本3。

训练的目标非常朴素，期望让相同类别间的距离尽可能的小，让不同类别间的距离尽可能的大，即减小类内距，增大类间距。

![img](https://pic4.zhimg.com/v2-f81fe86428117f6d8747a73fc0997e9f_b.png)

## **2、Sentence-BERT**

自从2018年底Bert等预训练语言模型横空出世，NLP届的游戏规则某种程度上被大幅更改了。在计算资源允许的条件下，Bert成为解决很多问题的首选。甚至有时候拿Bert跑一跑baseline，发现问题已经解决了十之八九。

但是Bert的**缺点**也很明显，1.1亿参数量使得推理速度明显比CNN等传统网络慢了不止一个量级，对资源要求更高，也不适合处理某些任务。

<img src="https://pic2.zhimg.com/v2-ff6d53a25ade409411af39153246a161_b.png" alt="img" style="zoom:50%;" />

例如，从10,000条句子中找到最相似的一对句子，由于可能的组合众多，需要完成49,995,000次推理；在一块现代V100GPU上使用Bert计算，将消耗**65小时**。

考虑到孪生网络的简洁有效，有没有可能将它和Bert强强联合呢？

当然可以，这正是论文**《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》**的工作，首次提出了Sentence-Bert模型（以下简称SBert）。

SBert在众多文本匹配工作中（包括语义相似性、推理等）都取得了最优结果。更让人惊讶的是，前文所述的从10000条句子寻找最相似pair任务，SBert仅需**5秒**就能完成！

### **基于BERT的文本匹配**

让我们简短回顾此前Bert是怎么处理文本匹配任务的。

![img](https://pic2.zhimg.com/v2-e8cbbe69c867b499cab6bd0ce5212579_b.png)

常规做法是将匹配转换成二分类任务。输入的两个文本拼接成一个序列（中间用特殊符号“SEP”分割），经过12层或24层Transformer模块编码后，将输出层的字向量取平均或者取“CLS”位置的特征作为句向量，经softmax完成最终分类。

但是论文作者 Nils Reimers 在实验中指出，这样的做法产生的结果并不理想（至少在处理语义检索和聚类问题时是如此），甚至比Glove词向量取平均的效果还差。

![img](https://pic1.zhimg.com/v2-a51f8e7d6b382c91f46b716b9f79504c_b.png)

### **基于S-BERT的文本匹配**

为了让Bert更好地利用文本信息，作者们在论文中提出了如下的**SBert**模型。是不是非常眼熟？对，这不就是之前见过的孪生网络嘛！

![img](https://pic2.zhimg.com/v2-90515a97f95d6aa0bf649798f5cabaad_b.png)

SBert沿用了孪生网络的结构，文本Encoder部分用同一个Bert来处理。之后，作者分别实验了CLS-token和2种池化策略（Avg-Pooling、Mean-Pooling），对Bert输出的字向量进一步特征提取、压缩，得到u、v。关于u、v整合，作者提供了3种策略：

- 针对分类任务，将u、v拼接，接入全连接网络，经softmax分类输出；损失函数用交叉熵
- 直接计算、输出余弦相似度；训练损失函数采用均方根误差
- 如果输入的是三元组，论文种也给出了相应的损失函数

![img](https://pic3.zhimg.com/v2-e00a0e764b1624bd8c766f9cbaf762aa_b.png)

总的来说，SBert直接用Bert的原始权重初始化，在具体数据集上微调，训练过程和传统Siamse Network差异不大。

但是这种训练方式能让Bert更好的捕捉句子之间的关系，生成更优质的句向量。**在测试阶段，SBert直接使用余弦相似度来衡量两个句向量之间的相似度**，极大提升了推理速度。

### **实验为证**

作者在**7个**文本匹配相关的任务中做了对比实验，结果在其中**5个**任务上，SBert都有更优表现。

![img](https://pic3.zhimg.com/v2-5868d6c3e7993075011df973b6868cda_b.png)

作者还做了一些有趣的消融实验。

使用NLI和STS为代表的匹配数据集，在分类目标函数训练时，作者测试了不同的**整合策略**，结果显示**“(u, v, |u-v|)”**的组合效果最好。这里面最重要的部分是元素差：（|u - v|)。句向量之间的差异度量了两个句子嵌入维度间的距离，确保相似的pair更近，不同的pair更远。

![img](https://pic1.zhimg.com/v2-6c344224820bc99367dbf1ddb9035380_b.png)

文章最后，作者将SBert和传统方法做了对比。

![img](https://pic4.zhimg.com/v2-35e7118d8ce330f3fc9de14108fd8f47_b.png)

SBert的计算效率要更高。其中的**smart-batching**是一个小技巧。先**将输入的文本按长度排序**，这样同一个mini-batch的文本长度更加统一，padding时能显著减少填充的token。

### **线下实测**

我们将SBert模型在[天池—新冠疫情相似句对判定比赛](http://mp.weixin.qq.com/s?__biz=Mzg2OTUyMzg5OQ==&mid=2247490140&idx=1&sn=482a5eee2bd39bb29a2a37ea65c4c8ba&chksm=ce9a9288f9ed1b9e080fc12e835f1fd531a3b6315f87408f181a7615c624b16cadb4b9062aa0&scene=21#wechat_redirect)数据集上做了测试。经数据增强后，线下训练集和验证集分别是13,500和800条句子组合。预训练模型权重选择BERT_large。

最终SBert单模型在验证集上的准确率是**95.7%**。直接使用Bert微调准确率为**95.2%**。

## **3、小结**

本文介绍了文本匹配任务中常用的孪生网络，和在此基础上改进而来的Sentence-BERT模型。

Siamse Network 简洁的设计和平稳高效训练非常适合作为文本匹配任务的baseline模型。SBert则充分利用了孪生网络的优点和预训练模型的特征抽取优势，在众多匹配任务上取得了最优结果。

抛开具体任务，**SBert 可以帮助我们生成更好的句向量**，在一些任务上可能产生更优结果。**在推理阶段，SBert直接计算余弦相似度的方式，大大缩短了预测时间**，在语义检索、信息搜索等任务中预计会有不错表现。同时，得益于生成的高质量句嵌入特征，**SBert也非常适合做文本聚类、新FAQ发现等工作**。

如果您觉得这篇文章对你有帮助，欢迎**点赞**，让更多的人也能看到这篇内容 ❤️

关注公众号**「NLP情报局」**，每周一篇算法干货，带你旋转带你飞～

## **参考资料**

[1] [Siamese network 孪生神经网络--一个简单神奇的结构](https://www.jianshu.com/p/92d7f6eaacf5)

[2] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: https://arxiv.org/abs/1908.10084

[3] Sentence-BERT pytorch开源: https://github.com/UKPLab/sentence-transformers

[4] 文本匹配相关方向打卡点总结: https://mp.weixin.qq.com/s/Nlr-VbbfUahYjMNPhquH4w